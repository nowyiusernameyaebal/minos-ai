# Multi-stage Dockerfile for Indexer Service (Production)
# Optimized for high-throughput blockchain data processing

# =====================================
# Base Image with Security Hardening
# =====================================
FROM node:20-alpine AS base

# Install essential system dependencies
RUN apk update && apk upgrade && apk add --no-cache \
    dumb-init \
    curl \
    ca-certificates \
    tzdata \
    && rm -rf /var/cache/apk/* \
    && addgroup -g 1001 -S nodejs \
    && adduser -S nodejs -u 1001 -G nodejs -h /home/nodejs

# Set timezone and locale
ENV TZ=UTC \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

# Create app directory with proper permissions
WORKDIR /app
RUN chown nodejs:nodejs /app

# =====================================
# Dependencies Stage
# =====================================
FROM base AS dependencies

USER nodejs

# Copy package files
COPY --chown=nodejs:nodejs package*.json ./
COPY --chown=nodejs:nodejs pnpm-lock.yaml ./

# Install pnpm
RUN npm install -g pnpm@8.11.0

# Configure npm registry
RUN pnpm config set registry https://registry.npmjs.org/ \
    && pnpm config set store-dir /home/nodejs/.pnpm-store \
    && pnpm config set cache-dir /home/nodejs/.pnpm-cache

# Install dependencies
RUN --mount=type=cache,target=/home/nodejs/.pnpm-store,uid=1001,gid=1001 \
    pnpm install --prod --frozen-lockfile --prefer-offline

# =====================================
# Build Stage
# =====================================
FROM base AS builder

USER nodejs

# Copy source code and package files
COPY --chown=nodejs:nodejs package*.json ./
COPY --chown=nodejs:nodejs pnpm-lock.yaml ./
COPY --chown=nodejs:nodejs tsconfig.json ./
COPY --chown=nodejs:nodejs src ./src
COPY --chown=nodejs:nodejs schemas ./schemas

# Install build dependencies
RUN npm install -g pnpm@8.11.0

# Install all dependencies including dev deps
RUN --mount=type=cache,target=/home/nodejs/.pnpm-store,uid=1001,gid=1001 \
    pnpm install --frozen-lockfile --prefer-offline

# Build application
ENV NODE_ENV=production
RUN pnpm run build

# Prune development dependencies
RUN pnpm prune --prod --config.ignore-scripts=true

# =====================================
# Production Runtime Stage
# =====================================
FROM base AS production

USER nodejs

# Set production environment with indexer optimizations
ENV NODE_ENV=production \
    NODE_OPTIONS="--max-old-space-size=4096 --optimize-for-size --no-deprecation --expose-gc" \
    NPM_CONFIG_LOGLEVEL=warn \
    PORT=3001 \
    UV_THREADPOOL_SIZE=128

# Create necessary directories for indexer operations
RUN mkdir -p \
    /app/logs \
    /app/cache \
    /app/data \
    /app/snapshots \
    /app/temp \
    /app/metrics \
    && chown -R nodejs:nodejs \
    /app/logs \
    /app/cache \
    /app/data \
    /app/snapshots \
    /app/temp \
    /app/metrics

# Copy built application and dependencies
COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nodejs:nodejs /app/package.json ./package.json
COPY --from=builder --chown=nodejs:nodejs /app/schemas ./schemas

# Create production entrypoint script
RUN cat > /app/entrypoint.sh << 'EOF'
#!/bin/sh
set -e

# Color codes
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ Starting Minos Indexer (Production)${NC}"

# Environment validation
required_vars="DATABASE_URL REDIS_URL SOLANA_RPC_URL"
for var in $required_vars; do
    eval value=\$$var
    if [ -z "$value" ]; then
        echo -e "${RED}âŒ ERROR: $var is required${NC}"
        exit 1
    fi
done

# Set default values for indexer configuration
export INDEXER_BATCH_SIZE=${INDEXER_BATCH_SIZE:-100}
export INDEXER_INTERVAL_MS=${INDEXER_INTERVAL_MS:-100}
export INDEXER_CONCURRENT_PROCESSORS=${INDEXER_CONCURRENT_PROCESSORS:-20}
export INDEXER_MAX_RETRIES=${INDEXER_MAX_RETRIES:-5}
export INDEXER_RETRY_DELAY_MS=${INDEXER_RETRY_DELAY_MS:-2000}
export CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-3600}
export MAX_BLOCK_HISTORY=${MAX_BLOCK_HISTORY:-10000}

# Performance optimizations
export NODE_OPTIONS="$NODE_OPTIONS --max-http-header-size=8192"

# Wait for dependencies with timeout
wait_for_service() {
    local host=$1
    local port=$2
    local service=$3
    local timeout=${4:-90}
    
    echo -e "${YELLOW}â³ Waiting for $service at $host:$port...${NC}"
    
    for i in $(seq 1 $timeout); do
        if nc -z "$host" "$port" 2>/dev/null; then
            echo -e "${GREEN}âœ… $service is ready${NC}"
            return 0
        fi
        sleep 1
    done
    
    echo -e "${RED}âŒ Timeout waiting for $service${NC}"
    return 1
}

# Extract database and redis connection details
DB_HOST=$(echo $DATABASE_URL | sed -n 's|.*://[^@]*@\([^:]*\):\([0-9]*\)/.*|\1|p')
DB_PORT=$(echo $DATABASE_URL | sed -n 's|.*://[^@]*@[^:]*:\([0-9]*\)/.*|\1|p')
REDIS_HOST=$(echo $REDIS_URL | sed -n 's|.*://\([^:]*\):\([0-9]*\).*|\1|p')
REDIS_PORT=$(echo $REDIS_URL | sed -n 's|.*://[^:]*:\([0-9]*\).*|\1|p')

# Extract Solana RPC details
SOLANA_HOST=$(echo $SOLANA_RPC_URL | sed -n 's|.*://\([^:]*\):\([0-9]*\).*|\1|p')
SOLANA_PORT=$(echo $SOLANA_RPC_URL | sed -n 's|.*://[^:]*:\([0-9]*\).*|\1|p')

# Wait for dependencies
wait_for_service "$DB_HOST" "$DB_PORT" "PostgreSQL" 90
wait_for_service "$REDIS_HOST" "$REDIS_PORT" "Redis" 60

# Check Solana RPC availability
echo -e "${YELLOW}â³ Checking Solana RPC connectivity...${NC}"
for i in $(seq 1 60); do
    if curl -s --max-time 5 -X POST -H "Content-Type: application/json" \
        -d '{"jsonrpc":"2.0","id":1,"method":"getHealth"}' \
        $SOLANA_RPC_URL | grep -q "ok"; then
        echo -e "${GREEN}âœ… Solana RPC is ready${NC}"
        break
    fi
    sleep 2
done

# Initialize indexer state if needed
if [ "${INITIALIZE_INDEXER:-true}" = "true" ] && [ -f "/app/scripts/init-indexer.js" ]; then
    echo -e "${BLUE}ðŸ”§ Initializing indexer state...${NC}"
    node /app/scripts/init-indexer.js || {
        echo -e "${RED}âŒ Indexer initialization failed${NC}"
        exit 1
    }
fi

# Run database migrations if needed
if [ "${RUN_MIGRATIONS:-true}" = "true" ] && [ -f "/app/scripts/migrate.js" ]; then
    echo -e "${BLUE}ðŸ”„ Running database migrations...${NC}"
    node /app/scripts/migrate.js || {
        echo -e "${RED}âŒ Migration failed${NC}"
        exit 1
    }
fi

# Set up graceful shutdown handler
_term() {
    echo -e "${YELLOW}ðŸ›‘ Received SIGTERM, initiating graceful shutdown...${NC}"
    
    # Send signal to the main process
    kill -TERM "$child" 2>/dev/null
    
    # Wait for graceful shutdown with timeout
    for i in $(seq 1 30); do
        if ! kill -0 "$child" 2>/dev/null; then
            echo -e "${GREEN}âœ… Graceful shutdown complete${NC}"
            exit 0
        fi
        sleep 1
    done
    
    # Force kill if still running
    echo -e "${RED}âš ï¸  Force killing process${NC}"
    kill -KILL "$child" 2>/dev/null
    exit 1
}

trap _term SIGTERM SIGINT

# Set up memory monitoring
if [ "${ENABLE_MEMORY_MONITORING:-true}" = "true" ]; then
    (
        while true; do
            MEMORY_USAGE=$(ps -o %mem= -p $$)
            MEMORY_MB=$(echo $MEMORY_USAGE | awk '{print int($1/100*4096)}')
            if [ $MEMORY_MB -gt 3500 ]; then
                echo -e "${YELLOW}âš ï¸  High memory usage: ${MEMORY_MB}MB${NC}"
                # Trigger garbage collection
                kill -USR1 "$child" 2>/dev/null || true
            fi
            sleep 30
        done
    ) &
fi

# Set up disk space monitoring
if [ "${ENABLE_DISK_MONITORING:-true}" = "true" ]; then
    (
        while true; do
            DISK_USAGE=$(df /app | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ $DISK_USAGE -gt 85 ]; then
                echo -e "${YELLOW}âš ï¸  High disk usage: ${DISK_USAGE}%${NC}"
                # Clean up old cache files
                find /app/cache -name "*.cache" -mtime +1 -delete 2>/dev/null || true
            fi
            sleep 60
        done
    ) &
fi

# Start the indexer application
echo -e "${GREEN}ðŸš€ Starting indexer on port $PORT${NC}"
echo -e "${BLUE}ðŸ“Š Indexer Configuration:${NC}"
echo -e "  - Batch Size: $INDEXER_BATCH_SIZE"
echo -e "  - Interval: ${INDEXER_INTERVAL_MS}ms"
echo -e "  - Concurrent Processors: $INDEXER_CONCURRENT_PROCESSORS"
echo -e "  - Max Retries: $INDEXER_MAX_RETRIES"

node dist/index.js &
child=$!
wait "$child"
EOF

chmod +x /app/entrypoint.sh

# Create enhanced health check script for indexer
RUN cat > /app/healthcheck.sh << 'EOF'
#!/bin/sh
set -e

# Check main health endpoint
if ! curl -f --max-time 15 --connect-timeout 5 \
    http://localhost:${PORT:-3001}/health 2>/dev/null; then
    exit 1
fi

# Check database connectivity
if ! curl -f --max-time 10 \
    http://localhost:${PORT:-3001}/health/db 2>/dev/null; then
    exit 1
fi

# Check Redis connectivity
if ! curl -f --max-time 10 \
    http://localhost:${PORT:-3001}/health/redis 2>/dev/null; then
    exit 1
fi

# Check Solana RPC connectivity
if ! curl -f --max-time 15 \
    http://localhost:${PORT:-3001}/health/solana 2>/dev/null; then
    exit 1
fi

# Check indexer-specific metrics
INDEXER_STATUS=$(curl -s --max-time 10 \
    http://localhost:${PORT:-3001}/metrics | grep "indexer_blocks_processed" | head -1)
if [ -z "$INDEXER_STATUS" ]; then
    echo "No indexer metrics available"
    exit 1
fi

# Check memory usage
MEMORY_USAGE=$(ps -o %mem= -p 1 | awk '{print int($1)}')
if [ $MEMORY_USAGE -gt 95 ]; then
    echo "Critical memory usage: ${MEMORY_USAGE}%"
    exit 1
fi

# Check if indexer is stuck (no progress in last 5 minutes)
LAST_BLOCK=$(curl -s --max-time 5 \
    http://localhost:${PORT:-3001}/stats | grep -o '"lastProcessedBlock":[0-9]*' | cut -d':' -f2)
if [ -f /tmp/last_block_check ]; then
    PREV_BLOCK=$(cat /tmp/last_block_check)
    if [ "$LAST_BLOCK" = "$PREV_BLOCK" ]; then
        echo "Indexer appears stuck at block $LAST_BLOCK"
        exit 1
    fi
fi
echo "$LAST_BLOCK" > /tmp/last_block_check

exit 0
EOF

chmod +x /app/healthcheck.sh

# Create log rotation script optimized for indexer
RUN cat > /app/rotate-logs.sh << 'EOF'
#!/bin/sh
set -e

LOG_DIR="/app/logs"
MAX_SIZE="500M"
MAX_FILES=10

# Rotate application logs
for log_file in $(find $LOG_DIR -name "*.log" -type f); do
    if [ -f "$log_file" ] && [ $(stat -f%z "$log_file" 2>/dev/null || stat -c%s "$log_file") -gt 524288000 ]; then
        for i in $(seq $((MAX_FILES-1)) -1 1); do
            if [ -f "${log_file}.$i" ]; then
                mv "${log_file}.$i" "${log_file}.$((i+1))"
            fi
        done
        mv "$log_file" "${log_file}.1"
        touch "$log_file"
        chmod 644 "$log_file"
        
        # Compress old log files
        if [ -f "${log_file}.2" ]; then
            gzip "${log_file}.2" 2>/dev/null || true
        fi
    fi
done

# Clean up old cache files
find /app/cache -name "*.cache" -mtime +7 -delete 2>/dev/null || true
find /app/temp -name "*" -mtime +1 -delete 2>/dev/null || true

exit 0
EOF

chmod +x /app/rotate-logs.sh

# Set up cron for log rotation (every 6 hours)
RUN echo "0 */6 * * * /app/rotate-logs.sh" | crontab -

# Create performance monitoring script
RUN cat > /app/monitor-performance.sh << 'EOF'
#!/bin/sh
set -e

LOG_FILE="/app/logs/performance.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

# Get system metrics
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')
MEMORY_USAGE=$(free | grep Mem | awk '{print ($3/$2) * 100.0}')
DISK_USAGE=$(df /app | tail -1 | awk '{print $5}' | sed 's/%//')

# Get Node.js metrics
if curl -s http://localhost:3001/metrics > /dev/null 2>&1; then
    HEAP_USED=$(curl -s http://localhost:3001/metrics | grep "nodejs_heap_size_used_bytes" | head -1 | awk '{print $2}')
    HEAP_TOTAL=$(curl -s http://localhost:3001/metrics | grep "nodejs_heap_size_total_bytes" | head -1 | awk '{print $2}')
    
    if [ -n "$HEAP_USED" ] && [ -n "$HEAP_TOTAL" ]; then
        HEAP_USAGE=$(echo "scale=2; ($HEAP_USED / $HEAP_TOTAL) * 100" | bc)
    else
        HEAP_USAGE="N/A"
    fi
else
    HEAP_USAGE="N/A"
fi

# Log metrics
echo "$DATE,CPU:${CPU_USAGE}%,Memory:${MEMORY_USAGE}%,Disk:${DISK_USAGE}%,Heap:${HEAP_USAGE}%" >> $LOG_FILE

# Alert if metrics are critical
if [ $(echo "$CPU_USAGE > 90" | bc) -eq 1 ] || [ $(echo "$MEMORY_USAGE > 90" | bc) -eq 1 ]; then
    echo "$DATE - CRITICAL: High resource usage detected" >> /app/logs/alerts.log
fi

exit 0
EOF

chmod +x /app/monitor-performance.sh

# Set up performance monitoring (every 5 minutes)
RUN echo "*/5 * * * * /app/monitor-performance.sh" | crontab -

# Security hardening
RUN chown -R nodejs:nodejs /app \
    && chmod -R 755 /app \
    && chmod -R 644 /app/dist \
    && chmod +x /app/entrypoint.sh /app/healthcheck.sh /app/rotate-logs.sh /app/monitor-performance.sh

# Expose application port and metrics port
EXPOSE 3001 9090

# Add comprehensive labels for container management
LABEL maintainer="Minos AI Team <ops@minos.ai>" \
      version="1.0.0" \
      description="Minos AI Indexer Service - Production" \
      environment="production" \
      service="indexer" \
      build-date="$(date -u +'%Y-%m-%dT%H:%M:%SZ')" \
      schema-version="1.0" \
      vcs-url="https://github.com/minos-ai/minos-ai"

# Health check with indexer-specific settings
HEALTHCHECK --interval=60s --timeout=30s --start-period=120s --retries=3 \
    CMD ["./healthcheck.sh"]

# Use dumb-init for proper signal handling
ENTRYPOINT ["dumb-init", "--"]

# Start the indexer application
CMD ["./entrypoint.sh"]